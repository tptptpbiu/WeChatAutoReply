# CMAKE generated file: DO NOT EDIT!
# Generated by "Ninja" Generator, CMake Version 3.22

# This file contains all the build statements describing the
# compilation DAG.

# =============================================================================
# Write statements declared in CMakeLists.txt:
# 
# Which is the root file.
# =============================================================================

# =============================================================================
# Project: llama-jni
# Configurations: Debug
# =============================================================================

#############################################
# Minimal version of Ninja required by this file

ninja_required_version = 1.5


#############################################
# Set configuration variable for custom commands.

CONFIGURATION = Debug
# =============================================================================
# Include auxiliary files.


#############################################
# Include rules file.

include CMakeFiles/rules.ninja

# =============================================================================

#############################################
# Logical path to working directory; prefix for absolute paths.

cmake_ninja_workdir = /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/
# =============================================================================
# Object build statements for SHARED_LIBRARY target llama-jni


#############################################
# Order-only phony target for llama-jni

build cmake_object_order_depends_target_llama-jni: phony || cmake_object_order_depends_target_ggml cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu cmake_object_order_depends_target_llama

build CMakeFiles/llama-jni.dir/llama-jni.cpp.o: CXX_COMPILER__llama-jni_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/llama-jni.cpp || cmake_object_order_depends_target_llama-jni
  DEFINES = -DGGML_USE_CPU -Dllama_jni_EXPORTS
  DEP_FILE = CMakeFiles/llama-jni.dir/llama-jni.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/../../../../llama.cpp/include -I/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/../../../../llama.cpp/common -I/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/../../../../llama.cpp/ggml/include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = CMakeFiles/llama-jni.dir
  OBJECT_FILE_DIR = CMakeFiles/llama-jni.dir
  TARGET_COMPILE_PDB = CMakeFiles/llama-jni.dir/
  TARGET_PDB = /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.pdb


# =============================================================================
# Link build statements for SHARED_LIBRARY target llama-jni


#############################################
# Link the shared library /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so

build /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so: CXX_SHARED_LIBRARY_LINKER__llama-jni_Debug CMakeFiles/llama-jni.dir/llama-jni.cpp.o | llama.cpp/src/libllama.a llama.cpp/ggml/src/libggml.a llama.cpp/ggml/src/libggml-cpu.a llama.cpp/ggml/src/libggml-base.a || llama.cpp/ggml/src/libggml-base.a llama.cpp/ggml/src/libggml-cpu.a llama.cpp/ggml/src/libggml.a llama.cpp/src/libllama.a
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  LINK_FLAGS = -static-libstdc++ -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Wl,--gc-sections -Wl,--no-undefined -Qunused-arguments
  LINK_LIBRARIES = llama.cpp/src/libllama.a  -landroid  -llog  llama.cpp/ggml/src/libggml.a  llama.cpp/ggml/src/libggml-cpu.a  llama.cpp/ggml/src/libggml-base.a  -pthread  -lm  -ldl  -latomic -lm
  OBJECT_DIR = CMakeFiles/llama-jni.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libllama-jni.so
  SONAME_FLAG = -Wl,-soname,
  TARGET_COMPILE_PDB = CMakeFiles/llama-jni.dir/
  TARGET_FILE = /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so
  TARGET_PDB = /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.pdb


#############################################
# Utility command for edit_cache

build CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build edit_cache: phony CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build rebuild_cache: phony CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build list_install_components: phony


#############################################
# Utility command for install

build CMakeFiles/install.util: CUSTOM_COMMAND all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build install: phony CMakeFiles/install.util


#############################################
# Utility command for install/local

build CMakeFiles/install/local.util: CUSTOM_COMMAND all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build install/local: phony CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build CMakeFiles/install/strip.util: CUSTOM_COMMAND all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build install/strip: phony CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/edit_cache: phony llama.cpp/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/rebuild_cache: phony llama.cpp/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/install: phony llama.cpp/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/install/local: phony llama.cpp/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/install/strip: phony llama.cpp/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/edit_cache: phony llama.cpp/ggml/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/rebuild_cache: phony llama.cpp/ggml/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/install: phony llama.cpp/ggml/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/install/local: phony llama.cpp/ggml/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/install/strip: phony llama.cpp/ggml/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for STATIC_LIBRARY target ggml-base


#############################################
# Order-only phony target for ggml-base

build cmake_object_order_depends_target_ggml-base: phony || llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o: C_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml.c || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o: CXX_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml.cpp || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o: C_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-alloc.c || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o: CXX_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-backend.cpp || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o: CXX_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-opt.cpp || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o: CXX_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-threading.cpp || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o: C_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-quants.c || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o: CXX_COMPILER__ggml-base_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/gguf.cpp || cmake_object_order_depends_target_ggml-base
  DEFINES = -DGGML_COMMIT=\"4e182bf\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_VERSION=\"0.9.5\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target ggml-base


#############################################
# Link the static library llama.cpp/ggml/src/libggml-base.a

build llama.cpp/ggml/src/libggml-base.a: CXX_STATIC_LIBRARY_LINKER__ggml-base_Debug llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-base.pdb
  TARGET_FILE = llama.cpp/ggml/src/libggml-base.a
  TARGET_PDB = llama.cpp/ggml/src/libggml-base.pdb

# =============================================================================
# Object build statements for STATIC_LIBRARY target ggml


#############################################
# Order-only phony target for ggml

build cmake_object_order_depends_target_ggml: phony || cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu

build llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o: CXX_COMPILER__ggml_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-backend-dl.cpp || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o: CXX_COMPILER__ggml_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-backend-reg.cpp || cmake_object_order_depends_target_ggml
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target ggml


#############################################
# Link the static library llama.cpp/ggml/src/libggml.a

build llama.cpp/ggml/src/libggml.a: CXX_STATIC_LIBRARY_LINKER__ggml_Debug llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o || llama.cpp/ggml/src/libggml-base.a llama.cpp/ggml/src/libggml-cpu.a
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml.pdb
  TARGET_FILE = llama.cpp/ggml/src/libggml.a
  TARGET_PDB = llama.cpp/ggml/src/libggml.pdb

# =============================================================================
# Object build statements for STATIC_LIBRARY target ggml-cpu


#############################################
# Order-only phony target for ggml-cpu

build cmake_object_order_depends_target_ggml-cpu: phony || cmake_object_order_depends_target_ggml-base

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o: C_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/repack.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/hbm.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o: C_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/quants.c || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/traits.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/vec.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/ops.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/quants.c.o: C_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/arch/arm/quants.c || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/quants.c.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -fno-limit-debug-info  -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -std=gnu11
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/repack.cpp.o: CXX_COMPILER__ggml-cpu_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/arch/arm/repack.cpp || cmake_object_order_depends_target_ggml-cpu
  DEFINES = -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CPU_REPACK -D_GNU_SOURCE -D_XOPEN_SOURCE=600
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/repack.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/.. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target ggml-cpu


#############################################
# Link the static library llama.cpp/ggml/src/libggml-cpu.a

build llama.cpp/ggml/src/libggml-cpu.a: CXX_STATIC_LIBRARY_LINKER__ggml-cpu_Debug llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/arm/repack.cpp.o || llama.cpp/ggml/src/libggml-base.a
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu.pdb
  TARGET_FILE = llama.cpp/ggml/src/libggml-cpu.a
  TARGET_PDB = llama.cpp/ggml/src/libggml-cpu.pdb


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/src/edit_cache: phony llama.cpp/ggml/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/src/rebuild_cache: phony llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/src/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/src/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install: phony llama.cpp/ggml/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/src/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install/local: phony llama.cpp/ggml/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install/strip: phony llama.cpp/ggml/src/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/edit_cache: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/rebuild_cache: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/src/ggml-cpu/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install/local: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install/strip: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for STATIC_LIBRARY target llama


#############################################
# Order-only phony target for llama

build cmake_object_order_depends_target_llama: phony || cmake_object_order_depends_target_ggml cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu

build llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-adapter.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-arch.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-batch.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-chat.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-context.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-cparams.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-grammar.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-graph.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-hparams.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-impl.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-io.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-kv-cache.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-kv-cache-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-memory.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-memory-hybrid.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-memory-hybrid-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-memory-recurrent.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-mmap.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-model-loader.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-model-saver.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-model.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-quant.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-sampler.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/llama-vocab.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/unicode-data.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/unicode.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/afmoe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/apertus.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/arcee.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/arctic.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/arwkv7.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/baichuan.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/bailingmoe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/bailingmoe2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/bert.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/bitnet.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/bloom.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/chameleon.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/chatglm.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/codeshell.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/cogvlm.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/cohere2-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/command-r.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/dbrx.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/deci.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/deepseek.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/deepseek2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/dots1.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/dream.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/ernie4-5-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/ernie4-5.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/exaone.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/exaone4.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/exaone-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/falcon-h1.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/falcon.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gemma-embedding.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gemma.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gemma2-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gemma3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gemma3n-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/glm4-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/glm4.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gpt2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/gptneox.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/granite-hybrid.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/granite.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/grok.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/grovemoe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/hunyuan-dense.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/hunyuan-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/internlm2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/jais.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/jamba.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/kimi-linear.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/lfm2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/llada-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/llada.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/llama-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/llama.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/maincoder.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/mamba.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/mimo2-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/minicpm3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/minimax-m2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/modern-bert.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/mpt.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/nemotron-h.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/nemotron.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/neo-bert.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/olmo.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/olmo2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/olmoe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/openai-moe-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/openelm.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/orion.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/pangu-embedded.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/phi2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/phi3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/plamo.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/plamo2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/plamo3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/plm.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen2moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen2vl.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen3vl.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen3vl-moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen3moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen3next.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen35.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/qwen35moe.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/refact.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rnd1.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rwkv6-base.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rwkv6.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rwkv6qwen2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rwkv7-base.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/rwkv7.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/seed-oss.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/smallthinker.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/smollm3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/stablelm.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/starcoder.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/starcoder2.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/step35-iswa.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/t5-dec.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/t5-enc.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/wavtokenizer-dec.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/xverse.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/mistral3.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb

build llama.cpp/src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o: CXX_COMPILER__llama_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/models/graph-context-mamba.cpp || cmake_object_order_depends_target_llama
  DEFINES = -DGGML_USE_CPU
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_PDB = llama.cpp/src/libllama.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target llama


#############################################
# Link the static library llama.cpp/src/libllama.a

build llama.cpp/src/libllama.a: CXX_STATIC_LIBRARY_LINKER__llama_Debug llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o || llama.cpp/ggml/src/libggml-base.a llama.cpp/ggml/src/libggml-cpu.a llama.cpp/ggml/src/libggml.a
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/src/CMakeFiles/llama.dir/llama.pdb
  TARGET_FILE = llama.cpp/src/libllama.a
  TARGET_PDB = llama.cpp/src/libllama.pdb


#############################################
# Utility command for edit_cache

build llama.cpp/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/src/edit_cache: phony llama.cpp/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/src/rebuild_cache: phony llama.cpp/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/src/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/src/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/src/install: phony llama.cpp/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/src/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/src/install/local: phony llama.cpp/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/src/install/strip: phony llama.cpp/src/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for OBJECT_LIBRARY target build_info


#############################################
# Order-only phony target for build_info

build cmake_object_order_depends_target_build_info: phony || llama.cpp/common/CMakeFiles/build_info.dir

build llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o: CXX_COMPILER__build_info_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common/build-info.cpp || cmake_object_order_depends_target_build_info
  DEP_FILE = llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi
  OBJECT_DIR = llama.cpp/common/CMakeFiles/build_info.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/build_info.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/build_info.dir/
  TARGET_PDB = ""



#############################################
# Object library build_info

build llama.cpp/common/build_info: phony llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o

# =============================================================================
# Object build statements for STATIC_LIBRARY target common


#############################################
# Order-only phony target for common

build cmake_object_order_depends_target_common: phony || cmake_object_order_depends_target_build_info cmake_object_order_depends_target_cpp-httplib cmake_object_order_depends_target_ggml cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu cmake_object_order_depends_target_llama

build llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/arg.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/chat-parser.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/chat-parser-xml-toolcall.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/chat-peg-parser.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/chat-peg-parser.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/chat-peg-parser.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/chat.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/common.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/common.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/console.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/console.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/debug.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/debug.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/debug.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/download.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/download.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/download.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/json-partial.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/json-schema-to-grammar.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/llguidance.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/log.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/log.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/log.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/ngram-cache.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/ngram-map.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/ngram-map.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/ngram-map.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/ngram-mod.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/ngram-mod.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/ngram-mod.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/peg-parser.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/peg-parser.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/peg-parser.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/preset.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/preset.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/preset.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/regex-partial.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/sampling.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/speculative.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/unicode.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/unicode.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/unicode.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/lexer.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/lexer.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/lexer.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/parser.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/parser.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/parser.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/runtime.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/runtime.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/runtime.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/value.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/value.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/value.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/string.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/string.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/string.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/jinja/caps.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/jinja/caps.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/jinja/caps.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/jinja
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb

build llama.cpp/common/CMakeFiles/common.dir/__/__/license.cpp.o: CXX_COMPILER__common_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/license.cpp || cmake_object_order_depends_target_common
  DEFINES = -DGGML_USE_CPU -DLLAMA_USE_HTTPLIB
  DEP_FILE = llama.cpp/common/CMakeFiles/common.dir/__/__/license.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -pthread -std=gnu++17
  INCLUDES = -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/. -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/../vendor -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/../include -I/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  OBJECT_FILE_DIR = llama.cpp/common/CMakeFiles/common.dir/__/__
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_PDB = llama.cpp/common/libcommon.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target common


#############################################
# Link the static library llama.cpp/common/libcommon.a

build llama.cpp/common/libcommon.a: CXX_STATIC_LIBRARY_LINKER__common_Debug llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o llama.cpp/common/CMakeFiles/common.dir/arg.cpp.o llama.cpp/common/CMakeFiles/common.dir/chat-parser.cpp.o llama.cpp/common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o llama.cpp/common/CMakeFiles/common.dir/chat-peg-parser.cpp.o llama.cpp/common/CMakeFiles/common.dir/chat.cpp.o llama.cpp/common/CMakeFiles/common.dir/common.cpp.o llama.cpp/common/CMakeFiles/common.dir/console.cpp.o llama.cpp/common/CMakeFiles/common.dir/debug.cpp.o llama.cpp/common/CMakeFiles/common.dir/download.cpp.o llama.cpp/common/CMakeFiles/common.dir/json-partial.cpp.o llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o llama.cpp/common/CMakeFiles/common.dir/llguidance.cpp.o llama.cpp/common/CMakeFiles/common.dir/log.cpp.o llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o llama.cpp/common/CMakeFiles/common.dir/ngram-map.cpp.o llama.cpp/common/CMakeFiles/common.dir/ngram-mod.cpp.o llama.cpp/common/CMakeFiles/common.dir/peg-parser.cpp.o llama.cpp/common/CMakeFiles/common.dir/preset.cpp.o llama.cpp/common/CMakeFiles/common.dir/regex-partial.cpp.o llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o llama.cpp/common/CMakeFiles/common.dir/speculative.cpp.o llama.cpp/common/CMakeFiles/common.dir/unicode.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/lexer.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/parser.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/runtime.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/value.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/string.cpp.o llama.cpp/common/CMakeFiles/common.dir/jinja/caps.cpp.o llama.cpp/common/CMakeFiles/common.dir/__/__/license.cpp.o || llama.cpp/common/build_info llama.cpp/ggml/src/libggml-base.a llama.cpp/ggml/src/libggml-cpu.a llama.cpp/ggml/src/libggml.a llama.cpp/src/libllama.a llama.cpp/vendor/cpp-httplib/libcpp-httplib.a
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/common/CMakeFiles/common.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/common/CMakeFiles/common.dir/common.pdb
  TARGET_FILE = llama.cpp/common/libcommon.a
  TARGET_PDB = llama.cpp/common/libcommon.pdb


#############################################
# Utility command for edit_cache

build llama.cpp/common/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/common/edit_cache: phony llama.cpp/common/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/common/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/common/rebuild_cache: phony llama.cpp/common/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/common/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/common/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/common/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/common/install: phony llama.cpp/common/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/common/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/common/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/common/install/local: phony llama.cpp/common/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/common/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/common/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/common/install/strip: phony llama.cpp/common/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for STATIC_LIBRARY target cpp-httplib


#############################################
# Order-only phony target for cpp-httplib

build cmake_object_order_depends_target_cpp-httplib: phony || llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir

build llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o: CXX_COMPILER__cpp-httplib_Debug /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/vendor/cpp-httplib/httplib.cpp || cmake_object_order_depends_target_cpp-httplib
  DEFINES = -DCPPHTTPLIB_FORM_URL_ENCODED_PAYLOAD_MAX_LENGTH=1048576 -DCPPHTTPLIB_LISTEN_BACKLOG=512 -DCPPHTTPLIB_REQUEST_URI_MAX_LENGTH=32768 -DCPPHTTPLIB_TCP_NODELAY=1
  DEP_FILE = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o.d
  FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info  -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -w -pthread -std=gnu++17
  OBJECT_DIR = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir
  OBJECT_FILE_DIR = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir
  TARGET_COMPILE_PDB = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/cpp-httplib.pdb
  TARGET_PDB = llama.cpp/vendor/cpp-httplib/libcpp-httplib.pdb


# =============================================================================
# Link build statements for STATIC_LIBRARY target cpp-httplib


#############################################
# Link the static library llama.cpp/vendor/cpp-httplib/libcpp-httplib.a

build llama.cpp/vendor/cpp-httplib/libcpp-httplib.a: CXX_STATIC_LIBRARY_LINKER__cpp-httplib_Debug llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o
  LANGUAGE_COMPILE_FLAGS = -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -fno-limit-debug-info
  OBJECT_DIR = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_COMPILE_PDB = llama.cpp/vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/cpp-httplib.pdb
  TARGET_FILE = llama.cpp/vendor/cpp-httplib/libcpp-httplib.a
  TARGET_PDB = llama.cpp/vendor/cpp-httplib/libcpp-httplib.pdb


#############################################
# Utility command for edit_cache

build llama.cpp/vendor/cpp-httplib/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/ccmake -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/vendor/cpp-httplib/edit_cache: phony llama.cpp/vendor/cpp-httplib/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/vendor/cpp-httplib/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake --regenerate-during-build -S/Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp -B/Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/vendor/cpp-httplib/rebuild_cache: phony llama.cpp/vendor/cpp-httplib/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/vendor/cpp-httplib/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/vendor/cpp-httplib/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/vendor/cpp-httplib/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/vendor/cpp-httplib/install: phony llama.cpp/vendor/cpp-httplib/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/vendor/cpp-httplib/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/vendor/cpp-httplib/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/vendor/cpp-httplib/install/local: phony llama.cpp/vendor/cpp-httplib/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/vendor/cpp-httplib/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/vendor/cpp-httplib/all
  COMMAND = cd /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib && /Users/bianha/Library/Android/sdk/cmake/3.22.1/bin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/vendor/cpp-httplib/install/strip: phony llama.cpp/vendor/cpp-httplib/CMakeFiles/install/strip.util

# =============================================================================
# Target aliases.

build build_info: phony llama.cpp/common/build_info

build common: phony llama.cpp/common/libcommon.a

build cpp-httplib: phony llama.cpp/vendor/cpp-httplib/libcpp-httplib.a

build ggml: phony llama.cpp/ggml/src/libggml.a

build ggml-base: phony llama.cpp/ggml/src/libggml-base.a

build ggml-cpu: phony llama.cpp/ggml/src/libggml-cpu.a

build libcommon.a: phony llama.cpp/common/libcommon.a

build libcpp-httplib.a: phony llama.cpp/vendor/cpp-httplib/libcpp-httplib.a

build libggml-base.a: phony llama.cpp/ggml/src/libggml-base.a

build libggml-cpu.a: phony llama.cpp/ggml/src/libggml-cpu.a

build libggml.a: phony llama.cpp/ggml/src/libggml.a

build libllama-jni.so: phony /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so

build libllama.a: phony llama.cpp/src/libllama.a

build llama: phony llama.cpp/src/libllama.a

build llama-jni: phony /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so

# =============================================================================
# Folder targets.

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a

build all: phony /Users/bianha/Desktop/chat/WeChatAutoReply/app/build/intermediates/cxx/Debug/7124281r/obj/arm64-v8a/libllama-jni.so llama.cpp/all

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp

build llama.cpp/all: phony llama.cpp/ggml/all llama.cpp/src/all llama.cpp/common/all llama.cpp/vendor/cpp-httplib/all

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/common

build llama.cpp/common/all: phony llama.cpp/common/build_info llama.cpp/common/libcommon.a

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml

build llama.cpp/ggml/all: phony llama.cpp/ggml/src/all

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src

build llama.cpp/ggml/src/all: phony llama.cpp/ggml/src/libggml-base.a llama.cpp/ggml/src/libggml.a llama.cpp/ggml/src/libggml-cpu.a llama.cpp/ggml/src/ggml-cpu/all

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/ggml/src/ggml-cpu

build llama.cpp/ggml/src/ggml-cpu/all: phony

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/src

build llama.cpp/src/all: phony llama.cpp/src/libllama.a

# =============================================================================

#############################################
# Folder: /Users/bianha/Desktop/chat/WeChatAutoReply/app/.cxx/Debug/7124281r/arm64-v8a/llama.cpp/vendor/cpp-httplib

build llama.cpp/vendor/cpp-httplib/all: phony llama.cpp/vendor/cpp-httplib/libcpp-httplib.a

# =============================================================================
# Built-in targets


#############################################
# Re-run CMake if any of its inputs changed.

build build.ninja: RERUN_CMAKE | /Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/build-info.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/common.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/license.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/llama-config.cmake.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/llama.pc.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/build-info.cpp.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/cmake/common.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/cmake/ggml-config.cmake.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/vendor/cpp-httplib/CMakeLists.txt /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/BasicConfigVersion-SameMajorVersion.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeASMCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeASMInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCCompilerABI.c /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXCompilerABI.cpp /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCommonLanguageInclude.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCompilerIdDetection.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineASMCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCXXCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompileFeatures.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompilerABI.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompilerId.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineSystem.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeFindBinUtils.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeGenericSystem.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeInitializeConfigs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeLanguageInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakePackageConfigHelpers.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseImplicitIncludeInfo.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseImplicitLinkInfo.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseLibraryArchitecture.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystem.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystemSpecificInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystemSpecificInitialize.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestASMCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCXXCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCompilerCommon.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCXXCompilerFlag.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCXXSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckForPthreads.c /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFile.c.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFile.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFileCXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckLibraryExists.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ADSP-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ARMCC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ARMClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/AppleClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Borland-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Bruce-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/CMakeCommonCompilerMacros.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-ASM.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-DetermineCompilerInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-FindBinUtils.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Comeau-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Compaq-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Compaq-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Cray-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Embarcadero-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Fujitsu-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/FujitsuClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GHS-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/HP-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/HP-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IAR-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IBMCPP-C-DetermineVersionInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IBMCPP-CXX-DetermineVersionInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Intel-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IntelLLVM-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/MSVC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/NVHPC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/NVIDIA-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/OpenWatcom-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/PGI-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/PathScale-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SCO-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SDCC-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SunPro-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SunPro-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/TI-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/TinyCC-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/VisualAge-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/VisualAge-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Watcom-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XL-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XL-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XLClang-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XLClang-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/zOS-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/zOS-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindGit.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindOpenSSL.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPackageMessage.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPkgConfig.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindThreads.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/GNUInstallDirs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/CheckCompilerFlag.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/CheckSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/FeatureTesting.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-ASM.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Initialize.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android/Determine-Compiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Linux.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/UnixPaths.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/WriteBasicConfigVersionFile.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/android-legacy.toolchain.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/android.toolchain.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/flags.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Clang.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Determine.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Initialize.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Determine-Compiler.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/platforms.cmake CMakeCache.txt CMakeFiles/3.22.1-g37088a8/CMakeASMCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeCCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeCXXCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeSystem.cmake
  pool = console


#############################################
# A missing CMake input file is not an error.

build /Users/bianha/Desktop/chat/WeChatAutoReply/app/src/main/cpp/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/build-info.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/common.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/license.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/llama-config.cmake.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/cmake/llama.pc.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/common/build-info.cpp.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/cmake/common.cmake /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/cmake/ggml-config.cmake.in /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/src/CMakeLists.txt /Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/vendor/cpp-httplib/CMakeLists.txt /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/BasicConfigVersion-SameMajorVersion.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeASMCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeASMInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCCompilerABI.c /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXCompiler.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXCompilerABI.cpp /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCXXInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCommonLanguageInclude.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeCompilerIdDetection.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineASMCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCXXCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompileFeatures.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompilerABI.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineCompilerId.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeDetermineSystem.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeFindBinUtils.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeGenericSystem.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeInitializeConfigs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeLanguageInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakePackageConfigHelpers.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseImplicitIncludeInfo.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseImplicitLinkInfo.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeParseLibraryArchitecture.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystem.cmake.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystemSpecificInformation.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeSystemSpecificInitialize.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestASMCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCXXCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CMakeTestCompilerCommon.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCXXCompilerFlag.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckCXXSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckForPthreads.c /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFile.c.in /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFile.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckIncludeFileCXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/CheckLibraryExists.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ADSP-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ARMCC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/ARMClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/AppleClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Borland-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Bruce-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/CMakeCommonCompilerMacros.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-ASM.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-DetermineCompilerInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang-FindBinUtils.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Clang.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Comeau-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Compaq-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Compaq-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Cray-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Embarcadero-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Fujitsu-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/FujitsuClang-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GHS-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/GNU.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/HP-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/HP-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IAR-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IBMCPP-C-DetermineVersionInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IBMCPP-CXX-DetermineVersionInternal.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Intel-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/IntelLLVM-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/MSVC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/NVHPC-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/NVIDIA-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/OpenWatcom-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/PGI-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/PathScale-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SCO-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SDCC-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SunPro-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/SunPro-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/TI-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/TinyCC-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/VisualAge-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/VisualAge-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/Watcom-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XL-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XL-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XLClang-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/XLClang-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/zOS-C-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Compiler/zOS-CXX-DetermineCompiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindGit.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindOpenSSL.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPackageMessage.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindPkgConfig.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/FindThreads.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/GNUInstallDirs.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/CheckCompilerFlag.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/CheckSourceCompiles.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Internal/FeatureTesting.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-ASM.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Clang.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine-C.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine-CXX.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Determine.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android-Initialize.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Android/Determine-Compiler.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/Linux.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/Platform/UnixPaths.cmake /Users/bianha/Library/Android/sdk/cmake/3.22.1/share/cmake-3.22/Modules/WriteBasicConfigVersionFile.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/android-legacy.toolchain.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/android.toolchain.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/flags.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Clang.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Determine.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android-Initialize.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Android.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/hooks/pre/Determine-Compiler.cmake /Users/bianha/Library/Android/sdk/ndk/25.1.8937393/build/cmake/platforms.cmake CMakeCache.txt CMakeFiles/3.22.1-g37088a8/CMakeASMCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeCCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeCXXCompiler.cmake CMakeFiles/3.22.1-g37088a8/CMakeSystem.cmake: phony


#############################################
# Clean all the built files.

build clean: CLEAN


#############################################
# Print all primary targets available.

build help: HELP


#############################################
# Make the all target the default.

default all
