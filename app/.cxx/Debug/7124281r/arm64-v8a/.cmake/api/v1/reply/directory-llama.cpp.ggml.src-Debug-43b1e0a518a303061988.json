{
	"backtraceGraph" : 
	{
		"commands" : 
		[
			"install",
			"ggml_add_backend_library",
			"ggml_add_cpu_backend_variant_impl"
		],
		"files" : 
		[
			"/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/CMakeLists.txt",
			"/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt"
		],
		"nodes" : 
		[
			{
				"file" : 0
			},
			{
				"command" : 2,
				"file" : 0,
				"line" : 445,
				"parent" : 0
			},
			{
				"command" : 1,
				"file" : 1,
				"line" : 22,
				"parent" : 1
			},
			{
				"command" : 0,
				"file" : 0,
				"line" : 262,
				"parent" : 2
			}
		]
	},
	"installers" : 
	[
		{
			"backtrace" : 3,
			"component" : "Unspecified",
			"destination" : "lib",
			"paths" : 
			[
				"llama.cpp/ggml/src/libggml-cpu.a"
			],
			"targetId" : "ggml-cpu::@94de48a0d5830e0c5d2f",
			"targetIndex" : 5,
			"type" : "target"
		}
	],
	"paths" : 
	{
		"build" : "llama.cpp/ggml/src",
		"source" : "/Users/bianha/Desktop/chat/WeChatAutoReply/llama.cpp/ggml/src"
	}
}
